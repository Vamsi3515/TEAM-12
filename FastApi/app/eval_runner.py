"""CLI runner for AI agent evaluations.

Run evaluations from command line with various options:
- python -m app.eval_runner --agent ats
- python -m app.eval_runner --agent github --save-results
- python -m app.eval_runner --all --compare-with-baseline
"""

import argparse
import asyncio
import json
import sys
from pathlib import Path
from datetime import datetime
from typing import Optional

# Add parent directory to path for imports
sys.path.insert(0, str(Path(__file__).parent.parent))

import pytest
from app.core.Utils.eval_reporter import EvalReporter


class EvalRunner:
    """Evaluation runner with CLI interface."""
    
    def __init__(self, verbose: bool = False):
        """Initialize runner.
        
        Args:
            verbose: Whether to print verbose output
        """
        self.verbose = verbose
        self.results_dir = Path(__file__).parent.parent / "eval_results"
        self.results_dir.mkdir(parents=True, exist_ok=True)
        self.reporter = EvalReporter(self.results_dir)
    
    def run_ats_evals(self, save_results: bool = True) -> dict:
        """Run ATS agent evaluations.
        
        Args:
            save_results: Whether to save results to disk
            
        Returns:
            Evaluation results dictionary
        """
        print("ğŸš€ Running ATS Agent Evaluations...")
        print("=" * 60)
        
        # Run pytest for ATS tests
        test_file = Path(__file__).parent.parent / "tests" / "test_evals.py"
        json_report_path = self.results_dir / f"pytest_report_ats_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        
        # Prepare pytest arguments with JSON report
        pytest_args = [
            str(test_file),
            "-v",
            "-k", "TestATSAgent",
            "--tb=short",
            "-p", "no:warnings",
            f"--json-report",
            f"--json-report-file={json_report_path}",
            "--json-report-indent=2"
        ]
        
        if self.verbose:
            pytest_args.append("-s")
        
        # Run tests and capture results
        exit_code = pytest.main(pytest_args)
        
        # Load the JSON report generated by pytest
        eval_results = {
            "agent": "ats",
            "timestamp": datetime.now().isoformat(),
            "test_cases": [],
            "exit_code": exit_code
        }
        
        if json_report_path.exists():
            with open(json_report_path, 'r', encoding='utf-8') as f:
                pytest_report = json.load(f)
                eval_results = self._parse_pytest_report(pytest_report, "ats")
            # Optionally remove temp JSON file
            # json_report_path.unlink()
        
        # Generate report if save_results is True
        if save_results:
            print("\nğŸ“Š Generating evaluation report...")
            report = self.reporter.generate_report(eval_results, "ats", save=True)
            print(f"âœ… Report saved to: {report.get('report_path')}")
            print(f"ğŸ“„ HTML report: {report.get('html_report_path')}")
            return report
        
        return eval_results
    
    def _parse_pytest_report(self, pytest_report: dict, agent: str) -> dict:
        """Parse pytest JSON report into eval results format.
        
        Args:
            pytest_report: Raw pytest JSON report
            agent: Agent name
            
        Returns:
            Formatted evaluation results
        """
        test_cases = []
        
        for test in pytest_report.get("tests", []):
            # Extract properties recorded during test
            properties = {}
            for prop in test.get("user_properties", []):
                # Handle both dict format ({key: value}) and tuple format ([key, value])
                if isinstance(prop, dict):
                    properties.update(prop)
                elif isinstance(prop, (list, tuple)) and len(prop) == 2:
                    properties[prop[0]] = prop[1]
            
            test_case = {
                "test_case_name": properties.get("test_case_name", "unknown"),
                "description": properties.get("description", ""),
                "outcome": test.get("outcome", "unknown"),
                "_duration_ms": properties.get("duration_ms", test.get("call", {}).get("duration", 0) * 1000),
                "_result": {
                    "ats_score": properties.get("actual_score", 0)
                },
                "expected_score_range": {
                    "min": properties.get("expected_min", 0),
                    "max": properties.get("expected_max", 100)
                },
                "_metrics": {}
            }
            
            # Extract metrics from properties
            if "score_accuracy" in properties:
                test_case["_metrics"]["score_accuracy"] = {
                    "accuracy": properties["score_accuracy"]
                }
            
            for metric in ["rejection_reasons_match", "strengths_match", "issues_match", "suggestions_match"]:
                if metric in properties:
                    metric_name = metric.replace("_match", "")
                    test_case["_metrics"][metric_name] = {
                        "match_rate": properties[metric]
                    }
            
            if "structure_valid" in properties:
                test_case["_metrics"]["structure"] = {
                    "valid": properties["structure_valid"]
                }
            
            if "response_time_ok" in properties:
                test_case["_metrics"]["response_time"] = {
                    "passed": properties["response_time_ok"]
                }
            
            # Extract test case details from nodeid
            if "[" in test.get("nodeid", "") and "]" in test.get("nodeid", ""):
                idx_str = test["nodeid"].split("[")[1].split("]")[0]
                test_case["test_index"] = int(idx_str) if idx_str.isdigit() else idx_str
            
            # Add error message if test failed
            if test.get("outcome") == "failed" and "call" in test:
                call_info = test["call"]
                if "longrepr" in call_info:
                    test_case["error"] = call_info["longrepr"]
            
            test_cases.append(test_case)
        
        summary = pytest_report.get("summary", {})
        
        return {
            "agent": agent,
            "timestamp": datetime.now().isoformat(),
            "test_cases": test_cases,
            "summary": summary,
            "duration": pytest_report.get("duration", 0),
            "exit_code": pytest_report.get("exitcode", 1)
        }
    
    def run_github_evals(self, save_results: bool = True) -> dict:
        """Run GitHub agent evaluations.
        
        Args:
            save_results: Whether to save results to disk
            
        Returns:
            Evaluation results dictionary
        """
        print("ğŸš€ Running GitHub Agent Evaluations...")
        print("=" * 60)
        
        # Run pytest for GitHub tests
        test_file = Path(__file__).parent.parent / "tests" / "test_evals.py"
        
        pytest_args = [
            str(test_file),
            "-v",
            "-k", "TestGitHubAgent",
            "--tb=short",
            "-p", "no:warnings"
        ]
        
        if self.verbose:
            pytest_args.append("-s")
        
        result = pytest.main(pytest_args)
        
        eval_results = {
            "agent": "GitHub",
            "timestamp": datetime.now().isoformat(),
            "test_cases": [],
            "exit_code": result
        }
        
        if save_results:
            print("\nğŸ“Š Generating evaluation report...")
            report = self.reporter.generate_report(eval_results, "github", save=True)
            print(f"âœ… Report saved to: {report.get('report_path')}")
            print(f"ğŸ“„ HTML report: {report.get('html_report_path')}")
            return report
        
        return eval_results
    
    def run_authenticity_evals(self, save_results: bool = True) -> dict:
        """Run Authenticity agent evaluations.
        
        Args:
            save_results: Whether to save results to disk
            
        Returns:
            Evaluation results dictionary
        """
        print("ğŸš€ Running Experience Authenticity Agent Evaluations...")
        print("=" * 60)
        
        # Run pytest for Authenticity tests
        test_file = Path(__file__).parent.parent / "tests" / "test_evals.py"
        json_report_path = self.results_dir / f"pytest_report_authenticity_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        
        # Prepare pytest arguments with JSON report
        pytest_args = [
            str(test_file),
            "-v",
            "-k", "TestAuthenticityAgent",
            "--tb=short",
            "-p", "no:warnings",
            f"--json-report",
            f"--json-report-file={json_report_path}",
            "--json-report-indent=2"
        ]
        
        if self.verbose:
            pytest_args.append("-s")
        
        # Run tests and capture results
        exit_code = pytest.main(pytest_args)
        
        # Load the JSON report generated by pytest
        eval_results = {
            "agent": "authenticity",
            "timestamp": datetime.now().isoformat(),
            "test_cases": [],
            "exit_code": exit_code
        }
        
        if json_report_path.exists():
            with open(json_report_path, 'r', encoding='utf-8') as f:
                pytest_report = json.load(f)
                eval_results = self._parse_pytest_report(pytest_report, "authenticity")
        
        # Generate report if save_results is True
        if save_results:
            print("\nğŸ“Š Generating evaluation report...")
            report = self.reporter.generate_report(eval_results, "authenticity", save=True)
            print(f"âœ… Report saved to: {report.get('report_path')}")
            print(f"ğŸ“„ HTML report: {report.get('html_report_path')}")
            return report
        
        return eval_results

    def run_learning_evals(self, save_results: bool = True) -> dict:
        """Run Learning Flow generator evaluations.

        Args:
            save_results: Whether to save results to disk

        Returns:
            Evaluation results dictionary
        """
        print("ğŸš€ Running Learning Flow Evaluations...")
        print("=" * 60)

        # Run pytest for Learning Flow tests
        test_file = Path(__file__).parent.parent / "tests" / "test_evals.py"
        json_report_path = self.results_dir / f"pytest_report_learning_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"

        pytest_args = [
            str(test_file),
            "-v",
            "-k", "TestLearningFlowGenerator",
            "--tb=short",
            "-p", "no:warnings",
            f"--json-report",
            f"--json-report-file={json_report_path}",
            "--json-report-indent=2"
        ]

        if self.verbose:
            pytest_args.append("-s")

        exit_code = pytest.main(pytest_args)

        eval_results = {
            "agent": "learning",
            "timestamp": datetime.now().isoformat(),
            "test_cases": [],
            "exit_code": exit_code
        }

        if json_report_path.exists():
            with open(json_report_path, 'r', encoding='utf-8') as f:
                pytest_report = json.load(f)
                eval_results = self._parse_pytest_report(pytest_report, "learning")

        if save_results:
            print("\nğŸ“Š Generating evaluation report...")
            report = self.reporter.generate_report(eval_results, "learning", save=True)
            print(f"âœ… Report saved to: {report.get('report_path')}")
            print(f"ğŸ“„ HTML report: {report.get('html_report_path')}")
            return report

        return eval_results

    def run_security_evals(self, save_results: bool = True) -> dict:
        """Run Security analyzer evaluations.

        Args:
            save_results: Whether to save results to disk

        Returns:
            Evaluation results dictionary
        """
        print("ğŸš€ Running Security Agent Evaluations...")
        print("=" * 60)

        # Run pytest for Security tests
        test_file = Path(__file__).parent.parent / "tests" / "test_evals.py"
        json_report_path = self.results_dir / f"pytest_report_security_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"

        pytest_args = [
            str(test_file),
            "-v",
            "-k", "TestSecurityAgent",
            "--tb=short",
            "-p", "no:warnings",
            f"--json-report",
            f"--json-report-file={json_report_path}",
            "--json-report-indent=2"
        ]

        if self.verbose:
            pytest_args.append("-s")

        exit_code = pytest.main(pytest_args)

        eval_results = {
            "agent": "security",
            "timestamp": datetime.now().isoformat(),
            "test_cases": [],
            "exit_code": exit_code
        }

        if json_report_path.exists():
            with open(json_report_path, 'r', encoding='utf-8') as f:
                pytest_report = json.load(f)
                eval_results = self._parse_pytest_report(pytest_report, "security")

        if save_results:
            print("\nğŸ“Š Generating evaluation report...")
            report = self.reporter.generate_report(eval_results, "security", save=True)
            print(f"âœ… Report saved to: {report.get('report_path')}")
            print(f"ğŸ“„ HTML report: {report.get('html_report_path')}")
            return report

        return eval_results
    
    def run_all_evals(self, save_results: bool = True) -> dict:
        """Run all agent evaluations.
        
        Args:
            save_results: Whether to save results
            
        Returns:
            Combined results dictionary
        """
        print("ğŸš€ Running All Agent Evaluations...")
        print("=" * 60)
        
        ats_results = self.run_ats_evals(save_results)
        print("\n" + "=" * 60 + "\n")
        github_results = self.run_github_evals(save_results)
        
        combined = {
            "timestamp": datetime.now().isoformat(),
            "agents": {
                "ats": ats_results,
                "github": github_results
            }
        }
        
        return combined
    
    def compare_with_baseline(
        self,
        agent: str,
        baseline_path: Optional[Path] = None
    ) -> dict:
        """Compare current results with baseline.
        
        Args:
            agent: Agent name (ats or github)
            baseline_path: Path to baseline report (auto-detected if None)
            
        Returns:
            Comparison results
        """
        print(f"ğŸ“Š Comparing {agent.upper()} agent with baseline...")
        
        # Run current evaluation
        if agent == "ats":
            current_report = self.run_ats_evals(save_results=True)
        elif agent == "github":
            current_report = self.run_github_evals(save_results=True)
        else:
            raise ValueError(f"Unknown agent: {agent}")
        
        # Find baseline if not provided
        if baseline_path is None:
            baseline_path = self._find_latest_baseline(agent)
        
        if baseline_path is None or not baseline_path.exists():
            print("âš ï¸ No baseline found. This will be the baseline.")
            return {"status": "no_baseline", "current": current_report}
        
        # Compare
        print(f"ğŸ“ˆ Comparing with baseline: {baseline_path}")
        comparison = self.reporter.compare_with_baseline(current_report, baseline_path)
        
        # Print comparison results
        print("\n" + "=" * 60)
        print("COMPARISON RESULTS")
        print("=" * 60)
        print(f"Pass Rate Change: {comparison['pass_rate_change']:+.1%}")
        print(f"Accuracy Change: {comparison['accuracy_change']:+.1%}")
        
        if comparison['regression']:
            print("âŒ REGRESSION DETECTED - Performance decreased significantly")
            return {"status": "regression", "comparison": comparison, "exit_code": 1}
        elif comparison['improvement']:
            print("âœ… IMPROVEMENT - Performance increased significantly")
            return {"status": "improvement", "comparison": comparison, "exit_code": 0}
        else:
            print("â¡ï¸ STABLE - Performance within acceptable range")
            return {"status": "stable", "comparison": comparison, "exit_code": 0}
    
    def _find_latest_baseline(self, agent: str) -> Optional[Path]:
        """Find most recent baseline report for agent."""
        pattern = f"eval_{agent}_*.json"
        reports = list(self.results_dir.glob(pattern))
        
        if not reports:
            return None
        
        # Sort by modification time
        reports.sort(key=lambda p: p.stat().st_mtime, reverse=True)
        
        # Return second most recent (first is current run)
        return reports[1] if len(reports) > 1 else reports[0]
    
    def list_results(self):
        """List all evaluation results."""
        print("ğŸ“Š Evaluation Results")
        print("=" * 60)
        
        json_reports = list(self.results_dir.glob("eval_*.json"))
        html_reports = list(self.results_dir.glob("eval_*.html"))
        
        if not json_reports:
            print("No evaluation results found.")
            return
        
        print(f"\nFound {len(json_reports)} evaluation reports:\n")
        
        for report_path in sorted(json_reports, key=lambda p: p.stat().st_mtime, reverse=True):
            stat = report_path.stat()
            timestamp = datetime.fromtimestamp(stat.st_mtime)
            
            # Load report to get summary
            with open(report_path, 'r') as f:
                report = json.load(f)
            
            agent = report.get("agent", "unknown")
            pass_rate = report.get("statistics", {}).get("pass_rate", 0)
            
            print(f"ğŸ“„ {report_path.name}")
            print(f"   Agent: {agent}")
            print(f"   Pass Rate: {pass_rate:.1%}")
            print(f"   Date: {timestamp.strftime('%Y-%m-%d %H:%M:%S')}")
            print()


def main():
    """Main CLI entry point."""
    parser = argparse.ArgumentParser(
        description="Run AI agent evaluations",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  python -m app.eval_runner --agent ats
  python -m app.eval_runner --agent github --save-results
  python -m app.eval_runner --all
  python -m app.eval_runner --agent ats --compare-with-baseline
  python -m app.eval_runner --list
        """
    )
    
    parser.add_argument(
        "--agent",
        choices=["ats", "github", "authenticity", "learning", "security"],
        help="Which agent to evaluate"
    )
    
    parser.add_argument(
        "--all",
        action="store_true",
        help="Run evaluations for all agents"
    )
    
    parser.add_argument(
        "--save-results",
        action="store_true",
        default=True,
        help="Save evaluation results to disk (default: True)"
    )
    
    parser.add_argument(
        "--no-save",
        action="store_true",
        help="Don't save results to disk"
    )
    
    parser.add_argument(
        "--compare-with-baseline",
        action="store_true",
        help="Compare results with baseline and check for regression"
    )
    
    parser.add_argument(
        "--baseline-path",
        type=Path,
        help="Path to specific baseline report for comparison"
    )
    
    parser.add_argument(
        "--verbose",
        "-v",
        action="store_true",
        help="Verbose output"
    )
    
    parser.add_argument(
        "--list",
        action="store_true",
        help="List all evaluation results"
    )
    
    args = parser.parse_args()
    
    # Create runner
    runner = EvalRunner(verbose=args.verbose)
    
    # Handle list command
    if args.list:
        runner.list_results()
        return 0
    
    # Determine save_results flag
    save_results = args.save_results and not args.no_save
    
    try:
        # Run evaluations
        if args.compare_with_baseline:
            if not args.agent:
                print("âŒ Error: --compare-with-baseline requires --agent")
                return 1
            
            result = runner.compare_with_baseline(args.agent, args.baseline_path)
            return result.get("exit_code", 0)
        
        elif args.all:
            runner.run_all_evals(save_results)
            return 0
        
        elif args.agent == "ats":
            runner.run_ats_evals(save_results)
            return 0
        
        elif args.agent == "github":
            runner.run_github_evals(save_results)
            return 0
        
        elif args.agent == "authenticity":
            runner.run_authenticity_evals(save_results)
            return 0
        elif args.agent == "learning":
            runner.run_learning_evals(save_results)
            return 0
        elif args.agent == "security":
            runner.run_security_evals(save_results)
            return 0
        
        else:
            parser.print_help()
            return 1
    
    except KeyboardInterrupt:
        print("\n\nâš ï¸ Evaluation interrupted by user")
        return 130
    
    except Exception as e:
        print(f"\nâŒ Error: {str(e)}")
        if args.verbose:
            import traceback
            traceback.print_exc()
        return 1


if __name__ == "__main__":
    sys.exit(main())
