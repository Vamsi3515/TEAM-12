# ðŸŽ¯ AI Agent Evaluations - Simple Explanation for Judges

## What This Project Does

This is a **GenAI-powered platform** that helps students and developers understand why they get rejected from jobs and how to improve. It uses **6 specialized AI agents** to analyze:
- Resumes vs job descriptions (ATS Agent)
- GitHub profiles and code quality (GitHub Agent)
- Skills gaps, learning paths, security, and system design

## ðŸ§ª What Are "Evals" (Evaluations)?

**Evals = Automated Quality Tests for AI Agents**

Think of it like a **teacher grading AI homework**. We test if our AI agents give correct, helpful answers by:

1. **Creating test scenarios** (like exam questions)
2. **Running AI agents** on those scenarios  
3. **Comparing AI answers** to expected correct answers
4. **Measuring accuracy** with scores and metrics

### Why Evals Matter
- âœ… Proves AI agents work reliably
- âœ… Catches mistakes before users see them
- âœ… Shows continuous quality improvement
- âœ… Industry best practice (OpenAI, Anthropic, etc. all use evals)

---

## ðŸ“Š What We Test

### 1. **ATS Resume Agent** - 16 Test Cases

Tests if the AI correctly analyzes resumes:

| Test Scenario | What We Check |
|--------------|---------------|
| **Perfect candidate** (Senior dev with 8 years) | Score should be 85-100% |
| **Junior applying for senior role** | Score should be 15-35% (rejection expected) |
| **Missing required skills** | Should detect Python, Cloud missing |
| **Career changer** (Teacher â†’ Developer) | Should flag non-CS degree |
| **Overqualified candidate** | Should detect senior applying for junior |
| **International candidate** | Should catch visa/location issues |
| **Employment gaps** | Should identify 2+ year gaps |
| **Fraudulent resume** | Should detect fake experience |

**Metrics We Measure:**
- âœ… **Score Accuracy**: Is the match score in expected range?
- âœ… **Rejection Reasons**: Does AI identify correct issues?
- âœ… **Strengths Detection**: Does AI spot candidate's best qualities?
- âœ… **Suggestions Quality**: Are improvement tips helpful?

### 2. **GitHub Repository Agent** - 15 Test Cases

Tests if the AI correctly analyzes GitHub profiles:

| Test Scenario | What We Check |
|--------------|---------------|
| **Professional open-source project** | Score 80-95% (good documentation, tests, CI/CD) |
| **Personal learning project** | Score 40-60% (basic quality) |
| **Inactive repository** | Should flag "no recent activity" |
| **No documentation** | Should catch missing README |
| **No tests** | Should identify lack of testing |
| **Security issues** | Should detect exposed secrets/keys |

**Metrics We Measure:**
- âœ… **Quality Score**: Is GitHub profile professional?
- âœ… **Issue Detection**: Are problems identified correctly?
- âœ… **Recommendation Quality**: Are tips actionable?

---

## ðŸŽ¯ Our Current Results

### Status: **Setup Complete** âœ…

**What's Working:**
- âœ… **6/6 Smoke Tests Passing** - All components load correctly
- âœ… **31 Total Test Cases Ready** (16 ATS + 15 GitHub)
- âœ… **Evaluation Framework Built** - Automated testing system
- âœ… **GitHub Actions CI/CD** - Tests run automatically on every code change
- âœ… **HTML + JSON Reports** - Beautiful visual reports generated

**Current Results File:** `eval_ats_2025-12-13T18-23-34.json`
- Shows: "No tests were run" 
- **Why?** Tests need API keys to run full evaluations (GROQ_API_KEY)
- **What this means:** Framework is ready, just needs credentials to execute

---

## ðŸ“ˆ How To Read Evaluation Results

When evals run successfully, you get:

### 1. **Statistics Summary**
```
Total Tests: 16
Passed: 14 âœ…
Failed: 2 âŒ
Pass Rate: 87.5%
```

### 2. **Per-Test Results**

Each test shows:
- **Score Accuracy**: Did AI give correct score? (e.g., 92% vs expected 85-100%)
- **Keyword Match**: Did AI identify right issues? (e.g., found 8/10 expected rejection reasons)
- **Response Time**: How fast? (e.g., 1200ms)

### 3. **What Good Results Look Like**

| Metric | Good Score | What It Means |
|--------|-----------|---------------|
| **Pass Rate** | >85% | AI is reliable |
| **Score Accuracy** | >90% | Matches are accurate |
| **Keyword Recall** | >80% | Catches all issues |
| **Avg Response Time** | <2000ms | Fast enough for users |

---

## ðŸš€ How Judges Can Verify

### Option 1: View Test Cases (No API needed)
```bash
# See what we test
cat FastApi/tests/fixtures/ats_test_cases.json
cat FastApi/tests/fixtures/github_test_cases.json
```

### Option 2: Run Smoke Tests (No API needed)
```bash
cd FastApi
python -m pytest tests/test_simple_eval.py -v
```
Shows: 6/6 tests passing âœ…

### Option 3: Run Full Evals (Requires API key)
```bash
cd FastApi
python -m app.eval_runner --agent ats --save-results
# Opens HTML report in browser
```

---

## ðŸ’¡ Why This Demonstrates Technical Excellence

### 1. **Industry Best Practices**
- Uses same evaluation approach as OpenAI, Anthropic, Google
- Automated quality assurance
- Continuous integration (GitHub Actions)

### 2. **Measurable Quality**
- Not just "AI does something" - we **prove it works**
- Quantified metrics (accuracy, precision, recall)
- Regression detection (catches if AI gets worse)

### 3. **Production-Ready**
- Tests run automatically on every code change
- Prevents bad code from being deployed
- Ensures consistent user experience

### 4. **Scalability**
- Easy to add new test cases
- Automated reporting
- CI/CD pipeline integrated

---

## ðŸ“¦ Key Files for Judges to Review

| File | What It Shows |
|------|---------------|
| [`FastApi/tests/fixtures/ats_test_cases.json`](FastApi/tests/fixtures/ats_test_cases.json) | All 16 ATS test scenarios with expected results |
| [`FastApi/tests/fixtures/github_test_cases.json`](FastApi/tests/fixtures/github_test_cases.json) | All 15 GitHub test scenarios |
| [`FastApi/app/core/eval_metrics.py`](FastApi/app/core/eval_metrics.py) | How we calculate accuracy metrics |
| [`FastApi/README_EVALS.md`](FastApi/README_EVALS.md) | Complete evaluation documentation |
| [`.github/workflows/eval.yml`](.github/workflows/eval.yml) | Automated CI/CD pipeline |

---

## ðŸŽ“ Summary for Judges

**What we built:**  
An automated testing system that proves our AI agents give accurate, helpful feedback to job seekers.

**How it works:**  
We created 31 realistic test scenarios, run our AI agents on them, and measure if answers match expected results using industry-standard metrics.

**Why it matters:**  
This is how professional AI companies ensure quality. It shows we're not just building "cool demos" - we're building **reliable, production-ready AI systems**.

**Current status:**  
- âœ… Framework complete and working
- âœ… All 31 test cases defined
- âœ… Smoke tests passing (6/6)
- âœ… CI/CD pipeline configured
- â³ Full evals ready to run (just need API keys for demo)

**Bottom line:**  
We've implemented the same quality assurance approach used by leading AI companies, demonstrating we understand how to build trustworthy AI systems at scale.
